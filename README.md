<p align="center">
  <img src="assets/portada_sin_fondo.png" alt="Icono semÃ¡foro" height="400"/>
</p>

# ğŸ§  Â¿QuÃ© es?

Un sistema  de desarrollo de esquemas de control de trÃ¡fico que permite diseÃ±ar **semÃ¡foros inteligentes basados en valores** mediante aprendizaje por refuerzo, evaluando  su desempeÃ±o  en un entorno de simulaciÃ³n de trÃ¡fico urbano.

---

## ğŸ“ Resumen

Frente al desafÃ­o global de congestiÃ³n urbana, los **semÃ¡foros inteligentes** emergen como herramienta clave, combinando el procesamiento de datos en tiempo real con la toma de decisiones autÃ³noma, logrando reducir atascos y tiempos de espera.

El anÃ¡lisis de los sistemas adaptativos de control de trÃ¡fico constituye un Ã¡rea en pleno desarrollo, donde destacan especialmente las tÃ©cnicas de **aprendizaje por refuerzo**. Este paradigma permite a los agentes semafÃ³ricos descubrir de forma autÃ³noma polÃ­ticas Ã³ptimas, aprendiendo mediante la interacciÃ³n continua con el entorno vial, guiados por seÃ±ales de recompensa que evalÃºan su comportamiento.

La flexibilidad en el diseÃ±o de recompensas abre la oportunidad de incorporar **valores sociales** mÃ¡s allÃ¡ del objetivo tradicional de eficiencia, como **sostenibilidad**, **seguridad** o **equidad**. Surge asÃ­ el concepto de **semÃ¡foros inteligentes basados en valores**, sistemas cuyo comportamiento se guÃ­a explÃ­citamente por consideraciones sociales, priorizando selectivamente unos valores u otros segÃºn necesidades urbanas especÃ­ficas.

Este trabajo aborda el desafÃ­o del diseÃ±o e implementaciÃ³n de sistemas de control de trÃ¡fico basados en valores mediante un enfoque integral: primero, se desarrolla un entorno integrado de entrenamiento y evaluaciÃ³n de polÃ­ticas semafÃ³ricas utilizando el simulador de trÃ¡fico SUMO; segundo, se implementa un algoritmo de aprendizaje por refuerzo multiagente con seÃ±ales de recompensa especializadas, generando un **catÃ¡logo de esquemas de control** orientados a diferentes valores sociales. Este repertorio permite la selecciÃ³n dinÃ¡mica de polÃ­ticas segÃºn necesidades contextuales especÃ­ficas. Finalmente, se realiza una evaluaciÃ³n comparativa rigurosa entre estos modelos y sistemas tradicionales, de tiempos fijos y adaptativos basados en reglas heurÃ­sticas.

---

### ğŸ¯ Objetivos del   proyeceto

- Construir un entorno integrado (SUMO + Gymnasium) para simular y comparar polÃ­ticas de control semafÃ³rico basadas en **eficiencia**, **sostenibilidad** y **equidad modal**.
- Evaluar desde modelos clÃ¡sicos (**tiempos fijos**), pasando por sistemas **adaptativos basados en reglas heurÃ­sticas**, hasta agentes avanzados de **aprendizaje por refuerzo profundo**.
- Generar un catÃ¡logo de agentes RL con **perfiles de valores diferenciados** segÃºn el diseÃ±o de la recompensa.



---

## âš™ï¸ TecnologÃ­as utilizadas

- **Python 3.10+**
- [**SUMO**](https://www.eclipse.org/sumo/) â€“ Motor de simulaciÃ³n de trÃ¡fico
- [**Gymnasium**](https://gymnasium.farama.org/) â€“ Entorno RL personalizable
- **PyTorch** â€“ Entrenamiento de redes neuronales

---

## ğŸ—‚ï¸ Estructura del repositorio

```plaintext
ğŸ“ /agents           â†’ Implementaciones  de los  diferentes  tipos de agentes (tiempos fijos, heurÃ­sticos, de aprendizaje)
ğŸ“ /models           â†’ Modelos entrenados, con  diferentes perfiles  de valores
ğŸ“ /plots            â†’ GrÃ¡ficas generadas a partir de los modelos entrenados
ğŸ“ /simulation       â†’ Archivos de simulaciÃ³n utilizados en los experimentos: '.net`, `.rou` y `.sumoconfig`.
ğŸ“„ createRoutes.py   â†’ Script para crear flujos de vehÃ­culos aleatorios a partir de una red de trÃ¡fico
ğŸ“„ graphics.txt      â†’ Script para generar grÃ¡ficas relacionadas con el entrenamiento de agentes
ğŸ“„ SUMOextractor.py  â†’ Script  con  diferentes  mÃ©todos para extraer informaciÃ³n de archivos `.net` de la red de trÃ¡fico
ğŸ“„ TrafficEnvironment.py  â†’ ImplementaciÃ³n del entorno (Gymnasium) con el que interactÃºan los agentes
ğŸ“„ train.py          â†’ Entrenamiento de nuevos agentes 
ğŸ“„ test.py           â†’  EvaluaciÃ³n de agentes con mÃ©tricas basadas en valores
```
---

<h2>â“  Â¿CÃ³mo usar el sistema? <img src="assets/semaforo2.png" alt="icono_semÃ¡foro" width="80" style="vertical-align:middle;"></h2>



1ï¸âƒ£ **Configura SUMO**  
   - Descarga SUMO desde su sitio oficial: [https://www.eclipse.org/sumo/](https://www.eclipse.org/sumo/)  
   - Crea la variable de entorno `SUMO_HOME` que apunte a la carpeta raÃ­z de tu instalaciÃ³n de SUMO para evitar modificar el cÃ³digo. Si prefieres, puedes establecer directamente en el cÃ³digo la ruta a tu carpeta de SUMO (tendrÃ¡s que  modificarlo en los scripts de train.py, test.py y SUMOextractor).

2ï¸âƒ£ **Prepara el entorno Python**  
   - AsegÃºrate de tener Python 3.10+ instalado y un editor de cÃ³digo (como VSCode, PyCharm o similar).  
   - Instala las dependencias del proyecto con `pip install -r requirements.txt`.

3ï¸âƒ£ **Crea o utiliza una simulaciÃ³n de trÃ¡fico**  
   - Para crear tu propia red, abre **NetEdit** (se instala con SUMO) y diseÃ±a la red a tu gusto.  
   > Nota: Este proyecto considera carreteras con intersecciones de 4 carriles en ambas direcciones. Para otras topologÃ­as serÃ¡ necesario modificar el extractor SUMO o incluso el entorno, en caso de que cambien las observaciones o acciones disponibles.  
   - Para usar la  red utilizada  en  los experimentos, copia los archivos de la carpeta `simulations` y pÃ©galos en tu directorio de SUMO (tu `SUMO_HOME`).

4ï¸âƒ£ **Ejecuta y prueba**  
   Puedes optar por:

   ### ğŸ…°ï¸ Probar el catÃ¡logo de modelos existentes

   - Este catÃ¡logo incluye polÃ­ticas orientadas a diferentes valores sociales:  
     - Eficiencia (reduce tiempos de retraso)  
     - Equidad modal (favorece transporte pÃºblico)  
     - Sostenibilidad: climÃ¡tica (reduce el consumo de combustible y emisiones de CO2)
     - Sostenibilidad: calidad del aire (reduce emisiones NOx)


   A1. **Selecciona el modelo que deseas probar**  
   - Elige uno de los modelos preentrenados disponibles en el catÃ¡logo.

   A2. **Ejecuta el script `test.py`**  
   - Utiliza el script `test.py` seleccionando el nombre del modelo que quieres probar. Puedes ajustar el nÃºmero de episodios o activar/desactivar el modo grÃ¡fico modificando la opciÃ³n `-gui` en el comando `sumo_cmd`.

   A3. **Observa  la simulaciÃ³n**  
   - Se abrirÃ¡ la interfaz  grÃ¡fica de SUMO y podrÃ¡s  obeservar la  simulacion de trÃ¡fico controlada por los semaforos selccionados. Al final, obtendrÃ¡s mÃ©tricas de desempeÃ±o.

   ### ğŸ…±ï¸ Crear tus propios agentes
   
   B1. **Define tu nuevo tipo de agente**  
   - Crea una nueva clase de agente en la carpeta `agents/`, heredando de `LearningAgent` si serÃ¡ un agente de aprendizaje o de `BaseAgent` en otro caso.

   B2. **Implementa los mÃ©todos necesarios**  
   - Implementa el mÃ©todo `act` y, en caso de ser un agente de aprendizaje, el mÃ©todo `act_and_train`. Estos  mÃ©todos determinarÃ¡n la forma de actuar de tus  agentes, asÃ­ como su proceso de entrenamiento.
   
   B3. **Personaliza la seÃ±al de recompensa**  
   - Puedes modificar la funciÃ³n de recompensa que recibirÃ¡n los agentes accediendo al entorno (`TrafficEnvironment.py`) y modificando el mÃ©todo `get_reward`.
   
   B4. **Integra tu  nuevo  tipo de  agente en el script de entrenamiento**  
   - Una vez configurado todo esto, ve al script `train.py` e incorpora tu tipo de agente en el mÃ©todo `train`, en la parte donde se crean los agentes, dÃ¡ndole el nombre que prefieras.
   
   B5. **Ejecuta el entrenamiento**  
   - Ejecuta el entrenamiento, indicando el nombre con el que quieres guardar el modelo y, opcionalmente, el nÃºmero de episodios a ejecutar. Si no lo especificas, se utilizarÃ¡ el mÃ©todo de early stopping.
   
  B6. **Analiza los resultados**  
   - Espera al entrenamiento y obtendrÃ¡s las grÃ¡ficas: curva de aprendizaje y evoluciÃ³n de las diferentes mÃ©tricas. Tu modelo estarÃ¡ listo  para ser probado.
   
  B7. **Prueba tu modelo**
   - Vuelve a la opciÃ³n A para probar tu modelo.
---


